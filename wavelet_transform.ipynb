{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a46193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wevelet\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import TransformerMixin\n",
    "import pywt\n",
    "\n",
    "class WaveletPackage(TransformerMixin):\n",
    "  '''\n",
    "  Extracts Wavelet Package features.\n",
    "  The features are calculated by the energy of the recomposed signal\n",
    "  of the leaf nodes coefficients.\n",
    "  '''\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "  def transform(self, X, y=None):\n",
    "    def Energy(coeffs, k):\n",
    "      return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])\n",
    "    def getEnergy(wp):\n",
    "      coefs = np.asarray([n.data for n in wp.get_leaf_nodes(True)])\n",
    "      return np.asarray([Energy(coefs,i) for i in range(2**wp.maxlevel)])\n",
    "    return np.array([getEnergy(pywt.WaveletPacket(data=x, wavelet='db4',\n",
    "                                                  mode='symmetric', maxlevel=4)\n",
    "                                                  ) for x in X[:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1510682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import UORED\n",
    "\n",
    "domains = {\n",
    "    '1': ('H_1_0', 'I_1_1', 'O_6_1', 'B_11_1'),\n",
    "    '2': ('H_2_0', 'I_1_2', 'O_6_2', 'B_11_2'),\n",
    "    '3': ('H_3_0', 'I_2_1', 'O_7_1', 'B_12_1'),\n",
    "    '4': ('H_4_0', 'I_2_2', 'O_7_2', 'B_12_2'),\n",
    "    '5': ('H_5_0', 'I_3_1', 'O_8_1', 'B_13_1'),\n",
    "    '6': ('H_6_0', 'I_3_2', 'O_8_2', 'B_13_2'),\n",
    "    '7': ('H_7_0', 'I_4_1', 'O_9_1', 'B_14_1'),\n",
    "    '8': ('H_8_0', 'I_4_2', 'O_9_2', 'B_14_2'),\n",
    "    '9': ('H_9_0', 'I_5_1', 'O_10_1', 'B_15_1'),\n",
    "    '10': ('H_10_0', 'I_5_2', 'O_10_2', 'B_15_2'),\n",
    "}\n",
    "\n",
    "train_domain = [(1, 3, 5, 7), (1, 3, 5, 9), (1, 3, 7, 9), \n",
    "                (1, 5, 7, 9), (3, 5, 7, 9), (2, 4, 6, 8), \n",
    "                (2, 4, 6, 10), (2, 4, 8, 10), (2, 6, 8, 10), (4, 5, 8, 10)]\n",
    "test_domain = [9, 7, 5, 3, 1, 10, 8, 6, 4, 2]\n",
    "\n",
    "def get_signals(filepath, output_path, max_allowed_sample_size=420_000, segment_size=4096, transform=None, transform_segment=None):\n",
    "    dataset = UORED()\n",
    "    signals = []\n",
    "    signal = dataset.load_file(filepath)[0]\n",
    "    if transform is not None:\n",
    "        signal = transform(signal)\n",
    "    max_allowed_sample_size = len(signal)\n",
    "    num_segments = max_allowed_sample_size//segment_size\n",
    "    for i in range(num_segments):\n",
    "        segment = signal[i*(segment_size):(i+1)*segment_size]\n",
    "        if transform_segment is not None:\n",
    "            segment = transform_segment(segment)\n",
    "        signals.append(segment)\n",
    "    return signals\n",
    "\n",
    "classes = (\"B\", \"I\", \"N\", \"O\")\n",
    "dsname_target = \"uored_wt\"\n",
    "def create_wt_dataset():\n",
    "    root_dir = \"data/raw/uored\"\n",
    "    for i, tr_domain in enumerate(train_domain):\n",
    "        tr = [domains[str(t)] for t in tr_domain]\n",
    "        ts = domains[str(test_domain[i])]\n",
    "        print(f\"Round: {i+1}\")\n",
    "        # TRAIN\n",
    "        for domain in tr:              \n",
    "            output_dir = root_dir.replace('raw', 'processed').replace('uored', dsname_target) + f'/setup_{i+1}/train/'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            for basename in domain:\n",
    "                path = f\"{root_dir}/{basename}\"\n",
    "                output_path = os.path.join(output_dir, basename.replace('H', 'N'))\n",
    "                get_segments(path, output_path, segment_size=2048)\n",
    "        # TEST\n",
    "        for basename in ts: \n",
    "            output_dir = root_dir.replace('raw', 'processed').replace('uored', dsname_target) + f'/setup_{i+1}/test/'\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            path = f\"{root_dir}/{basename}\"\n",
    "            output_path = os.path.join(output_dir, basename if basename[0]!='H' else basename.replace('H', 'N'))\n",
    "            get_segments(path, output_path, segment_size=1750)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "239e6903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved: wp_features/setup_4/test.csv | rows=408, cols=10\n",
      "[OK] Saved: wp_features/setup_5/test.csv | rows=408, cols=10\n",
      "[OK] Saved: wp_features/setup_6/test.csv | rows=408, cols=10\n",
      "[OK] Saved: wp_features/setup_7/test.csv | rows=408, cols=10\n",
      "[OK] Saved: wp_features/setup_8/test.csv | rows=408, cols=10\n",
      "[OK] Saved: wp_features/setup_9/test.csv | rows=408, cols=10\n",
      "[OK] Saved: wp_features/setup_10/test.csv | rows=408, cols=10\n"
     ]
    }
   ],
   "source": [
    "# wp_from_filelist.py\n",
    "# ---------------------------------------------------------\n",
    "# Apply your WaveletPackage to a list of .npy files and save a CSV\n",
    "# No extra transforms/normalization. Exactly your class as provided.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pywt\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# ---------- WaveletPackage (UNCHANGED) ----------\n",
    "class WaveletPackage(TransformerMixin):\n",
    "  '''\n",
    "  Extracts Wavelet Package features.\n",
    "  The features are calculated by the energy of the recomposed signal\n",
    "  of the leaf nodes coefficients.\n",
    "  '''\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "  def transform(self, X, y=None):\n",
    "    def Energy(coeffs, k):\n",
    "      return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])\n",
    "    def getEnergy(wp):\n",
    "      coefs = np.asarray([n.data for n in wp.get_leaf_nodes(True)])\n",
    "      return np.asarray([Energy(coefs,i) for i in range(2**wp.maxlevel)])\n",
    "    return np.array([getEnergy(pywt.WaveletPacket(data=x, wavelet='db4',\n",
    "                                                  mode='symmetric', maxlevel=3)\n",
    "                                                  ) for x in X[:]])\n",
    "# ------------------------------------------------\n",
    "\n",
    "def run(params: dict):\n",
    "    \"\"\"\n",
    "    params expected keys:\n",
    "      - file_paths: list of str/Path to .npy files (1D signals)\n",
    "      - out_csv: output CSV path (str/Path)\n",
    "      - add_path: (optional, bool) include original file path column. Default: True\n",
    "      - add_label: (optional, bool) include 'label' = first char of filename. Default: True\n",
    "    \"\"\"\n",
    "    file_paths = [Path(p) for p in params[\"file_paths\"]]\n",
    "    out_csv = Path(params[\"out_csv\"])\n",
    "    add_path = params.get(\"add_path\", True)\n",
    "    add_label = params.get(\"add_label\", True)\n",
    "\n",
    "    # Load signals (expects 1D; non-1D are skipped)\n",
    "    signals, kept_files = [], []\n",
    "    for f in file_paths:\n",
    "        try:\n",
    "            x = np.load(f)\n",
    "            x = np.asarray(x).squeeze()   # keep it 1D if possible\n",
    "            if x.ndim != 1:\n",
    "                print(f\"[SKIP] {f} not 1D (shape={x.shape})\")\n",
    "                continue\n",
    "            signals.append(x)\n",
    "            kept_files.append(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f}: {e}\")\n",
    "\n",
    "    if not signals:\n",
    "        print(\"[WARN] No valid signals to process.\")\n",
    "        return\n",
    "\n",
    "    # Stack to (N, L) and apply WaveletPackage\n",
    "    X = np.stack(signals, axis=0)\n",
    "    wp = WaveletPackage()\n",
    "    F = wp.transform(X)  # shape (N, 2**maxlevel) -> (N, 16) with maxlevel=4\n",
    "\n",
    "    # Build DataFrame with features f0..fK\n",
    "    df = pd.DataFrame(F, columns=[f\"f{i}\" for i in range(F.shape[1])])\n",
    "\n",
    "    # Optional columns\n",
    "    if add_path:\n",
    "        df.insert(0, \"path\", [p.as_posix() for p in kept_files])\n",
    "    if add_label:\n",
    "        # Class is the first letter of the filename (e.g., B_..., I_..., N_..., O_...)\n",
    "        labels = [(p.stem[0].upper() if p.stem else \"\") for p in kept_files]\n",
    "        df.insert(1 if add_path else 0, \"label\", labels)\n",
    "\n",
    "    # Save CSV\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Saved: {out_csv} | rows={len(df)}, cols={len(df.columns)}\")\n",
    "\n",
    "\n",
    "# ------------- Example usage -------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mode = 'test'\n",
    "    for n_stp in range(4,11):            \n",
    "        root=f\"data/processed/uored_4096/setup_{n_stp}/{mode}\"\n",
    "        for file in os.listdir(root):\n",
    "            paths = [Path(f'{root}/{file}') for file in os.listdir(root)]\n",
    "        params = {\n",
    "            \"file_paths\": paths,\n",
    "            \"out_csv\": f\"wp_features/setup_{n_stp}/{mode}.csv\",\n",
    "            \"add_path\": True,\n",
    "            \"add_label\": True\n",
    "        }\n",
    "        run(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72944bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets…\n",
      "[INFO] Train shape: X=(1632, 16), y=(1632,)\n",
      "[INFO] Test  shape: X=(408, 16),  y=(408,)\n",
      "[INFO] Training RandomForest…\n",
      "[INFO] Evaluating…\n",
      "\n",
      "===== RESULTS =====\n",
      "Accuracy   : 0.6373\n",
      "F1-macro   : 0.5704\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[ 83  19   0   0]\n",
      " [  0 102   0   0]\n",
      " [  0  30  72   0]\n",
      " [  0   6  93   3]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B     1.0000    0.8137    0.8973       102\n",
      "           I     0.6497    1.0000    0.7876       102\n",
      "           N     0.4364    0.7059    0.5393       102\n",
      "           O     1.0000    0.0294    0.0571       102\n",
      "\n",
      "    accuracy                         0.6373       408\n",
      "   macro avg     0.7715    0.6373    0.5704       408\n",
      "weighted avg     0.7715    0.6373    0.5704       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_rf_wp.py\n",
    "# ---------------------------------------------------------\n",
    "# Train a Random Forest on WaveletPackage features\n",
    "# Train on:  wp_features/train.csv\n",
    "# Test on:   wp_features/test.csv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def load_xy(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # feature columns: all that start with 'f'\n",
    "    feat_cols = [c for c in df.columns if c.startswith('f')]\n",
    "    if not feat_cols:\n",
    "        raise ValueError(f\"No feature columns starting with 'f' in {csv_path}\")\n",
    "    # label column: prefer 'label', fallback to 'label_str'\n",
    "    if 'label' in df.columns:\n",
    "        y = df['label'].values\n",
    "    elif 'label_str' in df.columns:\n",
    "        y = df['label_str'].values\n",
    "    else:\n",
    "        raise ValueError(f\"No 'label' or 'label_str' column in {csv_path}\")\n",
    "    X = df[feat_cols].to_numpy(dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "def main():\n",
    "    train_csv = \"wp_features/train.csv\"\n",
    "    test_csv  = \"wp_features/test.csv\"\n",
    "\n",
    "    print(\"[INFO] Loading datasets…\")\n",
    "    X_train, y_train = load_xy(train_csv)\n",
    "    X_test,  y_test  = load_xy(test_csv)\n",
    "\n",
    "    print(f\"[INFO] Train shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "    print(f\"[INFO] Test  shape: X={X_test.shape},  y={y_test.shape}\")\n",
    "\n",
    "    # Random Forest — simples e direto\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Training RandomForest…\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"[INFO] Evaluating…\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average='macro')\n",
    "    cm  = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== RESULTS =====\")\n",
    "    print(f\"Accuracy   : {acc:.4f}\")\n",
    "    print(f\"F1-macro   : {f1m:.4f}\")\n",
    "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    # (Opcional) salvar o modelo:\n",
    "    # import joblib\n",
    "    # joblib.dump(clf, \"rf_wp_model.joblib\")\n",
    "    # print(\"[INFO] Saved model to rf_wp_model.joblib\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_wp_csv_dict.py\n",
    "# ---------------------------------------------------------\n",
    "# Build a CSV of WaveletPackage features from 1D .npy signals\n",
    "# Using parameters passed via a dictionary instead of argparse\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pywt\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# ---------- WaveletPackage (original) ----------\n",
    "class WaveletPackage(TransformerMixin):\n",
    "    '''\n",
    "    Extracts Wavelet Package features.\n",
    "    The features are calculated by the energy of the recomposed signal\n",
    "    of the leaf nodes coefficients.\n",
    "    '''\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        def Energy(coeffs, k):\n",
    "            return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])\n",
    "        def getEnergy(wp):\n",
    "            coefs = np.asarray([n.data for n in wp.get_leaf_nodes(True)])\n",
    "            return np.asarray([Energy(coefs,i) for i in range(2**wp.maxlevel)])\n",
    "        return np.array([getEnergy(pywt.WaveletPacket(data=x, wavelet='db4',\n",
    "                                                      mode='symmetric', maxlevel=4)\n",
    "                                                      ) for x in X[:]])\n",
    "# ------------------------------------------------\n",
    "\n",
    "def list_npy_files(root: Path):\n",
    "    return [p for p in root.rglob(\"*.npy\") if p.is_file()]\n",
    "\n",
    "def run(params: dict):\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"in_root\": \"data/uored\",\n",
    "        \"out_csv\": \"out/wp_features.csv\",\n",
    "        \"add_path\": True,\n",
    "        \"infer_label\": True\n",
    "    }\n",
    "    \"\"\"\n",
    "    in_root = Path(params[\"in_root\"]).resolve()\n",
    "    out_csv = Path(params[\"out_csv\"]).resolve()\n",
    "    add_path = params.get(\"add_path\", False)\n",
    "    infer_label = params.get(\"infer_label\", False)\n",
    "\n",
    "    files = list_npy_files(in_root)\n",
    "    if not files:\n",
    "        print(f\"[WARN] No .npy found under {in_root}\")\n",
    "        return\n",
    "\n",
    "    # Load signals\n",
    "    signals, valid_files = [], []\n",
    "    for f in files:\n",
    "        try:\n",
    "            x = np.load(f)\n",
    "            x = np.asarray(x).squeeze()\n",
    "            if x.ndim != 1:\n",
    "                print(f\"[SKIP] {f} not 1D (shape={x.shape})\")\n",
    "                continue\n",
    "            signals.append(x)\n",
    "            valid_files.append(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f}: {e}\")\n",
    "\n",
    "    if not signals:\n",
    "        print(\"[WARN] No valid signals loaded.\")\n",
    "        return\n",
    "\n",
    "    # Apply WaveletPackage\n",
    "    wp = WaveletPackage()\n",
    "    X = np.stack(signals, axis=0)\n",
    "    F = wp.transform(X)  # features (N, 16)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(F, columns=[f\"f{i}\" for i in range(F.shape[1])])\n",
    "\n",
    "    if add_path:\n",
    "        df.insert(0, \"path\", [p.as_posix() for p in valid_files])\n",
    "\n",
    "    if infer_label:\n",
    "        LABELS = {\"B\": 0, \"I\": 1, \"N\": 2, \"O\": 3}\n",
    "        labels = []\n",
    "        for p in valid_files:\n",
    "            lbl = None\n",
    "            for part in p.parts[::-1]:\n",
    "                up = part.upper()\n",
    "                if up in LABELS:\n",
    "                    lbl = LABELS[up]; break\n",
    "            if lbl is None and p.stem:\n",
    "                lbl = LABELS.get(p.stem[0].upper(), None)\n",
    "            labels.append(lbl)\n",
    "        df.insert(1 if add_path else 0, \"label\", labels)\n",
    "\n",
    "    # Save CSV\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[OK] Saved {out_csv} | rows={len(df)}, cols={len(df.columns)}\")\n",
    "\n",
    "\n",
    "# ---------------- EXAMPLE USO ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    params = {\n",
    "        \"in_root\": \"data/uored\",\n",
    "        \"out_csv\": \"out/wp_features.csv\",\n",
    "        \"add_path\": True,\n",
    "        \"infer_label\": True\n",
    "    }\n",
    "    run(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6432cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_xy(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    feat_cols = [c for c in df.columns if c.startswith('f')]\n",
    "    if not feat_cols:\n",
    "        raise ValueError(f\"No feature columns starting with 'f' in {csv_path}\")\n",
    "\n",
    "    # Prefer numeric label; fallback to string label\n",
    "    if 'label' in df.columns:\n",
    "        y = df['label'].values\n",
    "    elif 'label_str' in df.columns:\n",
    "        mapping = {s: i for i, s in enumerate(sorted(df['label_str'].astype(str).unique()))}\n",
    "        y = df['label_str'].map(mapping).values\n",
    "    else:\n",
    "        raise ValueError(f\"No 'label' or 'label_str' column in {csv_path}\")\n",
    "\n",
    "    X = df[feat_cols].to_numpy(dtype=np.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66be8412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.5009004 , 0.6179552 , 0.42690998, ..., 0.45193693, 0.35042167,\n",
       "         1.3690679 ],\n",
       "        [0.6017406 , 0.15788439, 0.10962422, ..., 0.20720935, 0.13360715,\n",
       "         0.7689403 ],\n",
       "        [1.2230427 , 0.6502544 , 0.46886766, ..., 0.338925  , 0.3303898 ,\n",
       "         1.3351505 ],\n",
       "        ...,\n",
       "        [9.006909  , 1.2786608 , 1.7502707 , ..., 2.1276722 , 1.2145617 ,\n",
       "         4.35925   ],\n",
       "        [1.7720951 , 1.2276839 , 0.48793954, ..., 0.3615816 , 0.29286718,\n",
       "         0.5441611 ],\n",
       "        [0.85376006, 0.9950107 , 0.86190337, ..., 0.4059483 , 0.59895235,\n",
       "         0.6419706 ]], dtype=float32),\n",
       " array(['O', 'B', 'O', ..., 'I', 'O', 'O'], dtype=object))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_xy(\"wp_features/setup_3/train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
